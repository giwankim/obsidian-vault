---
title: "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks"
source: "https://arxiv.org/abs/2602.12670"
author:
  - "[[Xiangyi Li]]"
  - "[[Wenbo Chen]]"
  - "[[Yimin Liu]]"
  - "[[Shenghan Zheng]]"
  - "[[Xiaokun Chen]]"
  - "[[Yifeng He]]"
  - "[[Yubo Li]]"
  - "[[Bingran You]]"
  - "[[Haotian Shen]]"
  - "[[Jiankai Sun]]"
published:
created: 2026-02-17
description: "Abstract page for arXiv paper 2602.12670: SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks"
tags:
  - "clippings"
---

> [!summary]
> SkillsBench is a benchmark of 86 tasks across 11 domains for evaluating "Agent Skills" â€” structured procedural knowledge packages that augment LLM agents at inference time. Testing 7 agent-model configurations over 7,308 trajectories, the study finds that curated skills raise average pass rate by 16.2 percentage points, though effects vary widely by domain. Notably, self-generated skills provide no benefit on average, and focused skills with 2-3 modules outperform comprehensive documentation.

\[Submitted on 13 Feb 2026\]

## Title:SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks

Authors:, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,

[View PDF](https://arxiv.org/pdf/2602.12670) [HTML (experimental)](https://arxiv.org/html/2602.12670v1)

> Abstract:Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

| Subjects: | Artificial Intelligence (cs.AI) |
| --- | --- |
| Cite as: | [arXiv:2602.12670](https://arxiv.org/abs/2602.12670) \[cs.AI\] |
|  | (or [arXiv:2602.12670v1](https://arxiv.org/abs/2602.12670v1) \[cs.AI\] for this version) |
|  | [https://doi.org/10.48550/arXiv.2602.12670](https://doi.org/10.48550/arXiv.2602.12670)  arXiv-issued DOI via DataCite (pending registration) |

## Submission history

From: Xiangyi Li \[[view email](https://arxiv.org/show-email/c4ce445a/2602.12670)\]
**\[v1\]** Fri, 13 Feb 2026 07:06:06 UTC (1,366 KB)

## Bibliographic and Citation Tools

Bibliographic Explorer *([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*

Connected Papers *([What is Connected Papers?](https://www.connectedpapers.com/about))*

Litmaps *([What is Litmaps?](https://www.litmaps.co/))*

scite Smart Citations *([What are Smart Citations?](https://www.scite.ai/))*

## Code, Data and Media Associated with this Article

alphaXiv *([What is alphaXiv?](https://alphaxiv.org/))*

CatalyzeX Code Finder for Papers *([What is CatalyzeX?](https://www.catalyzex.com/))*

DagsHub *([What is DagsHub?](https://dagshub.com/))*

Gotit.pub *([What is GotitPub?](http://gotit.pub/faq))*

Hugging Face *([What is Huggingface?](https://huggingface.co/huggingface))*

Papers with Code *([What is Papers with Code?](https://paperswithcode.com/))*

ScienceCast *([What is ScienceCast?](https://sciencecast.org/welcome))*

## Demos

Replicate *([What is Replicate?](https://replicate.com/docs/arxiv/about))*

Hugging Face Spaces *([What is Spaces?](https://huggingface.co/docs/hub/spaces))*

TXYZ.AI *([What is TXYZ.AI?](https://txyz.ai/))*

## arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2602.12670) | [Disable MathJax](https://arxiv.org/abs/) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))
